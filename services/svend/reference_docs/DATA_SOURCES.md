# Svend Training Data Sources

## Overview

Training data for Svend comes from curated open-source datasets. All sources are verified available on Hugging Face as of January 2025.

---

## Primary Training Datasets

### Math Reasoning

| Dataset | Size | Description | License | HF Link |
|---------|------|-------------|---------|---------|
| nvidia/OpenMathInstruct-2 | 14M pairs | Math problem-solution pairs generated by Llama3.1-405B. GSM8K/MATH augmentation. | CC-BY-4.0 | [Link](https://huggingface.co/datasets/nvidia/OpenMathInstruct-2) |
| camel-ai/math | 50K pairs | GPT-4 generated math problems across 25 topics x 25 subtopics | CC-BY-NC-4.0 | [Link](https://huggingface.co/datasets/camel-ai/math) |

### General Reasoning

| Dataset | Size | Description | License | HF Link |
|---------|------|-------------|---------|---------|
| Open-Orca/SlimOrca | 500K | Curated GPT-4 completions from FLAN, quality-filtered | MIT | [Link](https://huggingface.co/datasets/Open-Orca/SlimOrca) |
| kaist-ai/CoT-Collection | 1.88M | Chain-of-thought rationales across 1,060 tasks | Apache 2.0 | [Link](https://huggingface.co/datasets/kaist-ai/CoT-Collection) |

### Science Reasoning

| Dataset | Size | Description | License | HF Link |
|---------|------|-------------|---------|---------|
| camel-ai/physics | 20K pairs | GPT-4 physics problems, 25 topics x 25 subtopics | CC-BY-NC-4.0 | [Link](https://huggingface.co/datasets/camel-ai/physics) |
| camel-ai/chemistry | 20K pairs | GPT-4 chemistry problems, 25 topics x 25 subtopics | CC-BY-NC-4.0 | [Link](https://huggingface.co/datasets/camel-ai/chemistry) |

### Code Reasoning

| Dataset | Size | Description | License | HF Link |
|---------|------|-------------|---------|---------|
| TokenBender/code_instructions_122k_alpaca_style | 122K | Code instruction-following in Alpaca format | Apache 2.0 | [Link](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) |

---

## Additional High-Quality Sources

These are newer datasets discovered during research that may improve training:

| Dataset | Size | Description | License | HF Link |
|---------|------|-------------|---------|---------|
| open-thoughts/OpenThoughts3 | 1.2M | 850K math + 250K code + 100K science, QwQ-32B traces | Apache 2.0 | [Link](https://github.com/open-thoughts/open-thoughts) |
| Open-Orca/SlimOrca-Dedup | 363K | Deduplicated SlimOrca, cleaner | MIT | [Link](https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup) |

---

## Evaluation Benchmarks (not for training)

| Dataset | Size | Purpose | HF Link |
|---------|------|---------|---------|
| zhibei1204/PhysReason | 1.2K | Physics reasoning benchmark, 147 theorems | [Link](https://huggingface.co/datasets/zhibei1204/PhysReason) |
| jablonkagroup/ChemBench | 2.7K | Chemistry/materials science benchmark | [Link](https://huggingface.co/datasets/jablonkagroup/ChemBench) |
| UGPhysics/ugphysics | 5.5K | Undergraduate physics benchmark | [Link](https://huggingface.co/datasets/UGPhysics/ugphysics) |
| avaliev/ChemistryQA | 4.5K | Chemistry QA benchmark | [Link](https://huggingface.co/datasets/avaliev/ChemistryQA) |

---

## Proposed Data Mix

### Phase 1: Base Reasoning (no tools)

| Category | Dataset | Sample Size | % of Mix |
|----------|---------|-------------|----------|
| Math | OpenMathInstruct-2 | 100K | 40% |
| Reasoning | SlimOrca-Dedup | 50K | 20% |
| CoT | CoT-Collection | 50K | 20% |
| Code | code_instructions_122k | 25K | 10% |
| Science | camel-ai/physics + chemistry | 25K | 10% |
| **Total** | | **250K** | 100% |

### Phase 2: Tool Integration

| Category | Dataset | Sample Size | % of Mix |
|----------|---------|-------------|----------|
| Tool traces | Synthetic (Claude API) | 15K | 30% |
| Math | OpenMathInstruct-2 | 15K | 30% |
| Science | camel-ai/physics + chemistry | 10K | 20% |
| Code | code_instructions_122k | 10K | 20% |
| **Total** | | **50K** | 100% |

---

## Synthetic Data (to generate)

Tool-use training data will be generated using Claude API:

| Type | Target Size | Cost Estimate |
|------|-------------|---------------|
| Math tool calls (SymPy) | 5K | ~$20 |
| Code execution traces | 5K | ~$20 |
| Physics calculations | 3K | ~$15 |
| Chemistry calculations | 2K | ~$10 |
| **Total** | **15K** | **~$65** |

Schema defined in `scripts/generate_tool_data.py`.

---

## License Considerations

- **CC-BY-4.0**: Free to use commercially with attribution
- **CC-BY-NC-4.0**: Non-commercial only (camel-ai datasets)
- **Apache 2.0 / MIT**: Free to use commercially

For commercial deployment, may need to exclude or replace camel-ai datasets with alternatives.

---

## Loading Examples

```python
from datasets import load_dataset

# Large dataset - use streaming
math_data = load_dataset('nvidia/OpenMathInstruct-2', split='train_1M', streaming=True)

# Smaller datasets - load directly
slimorca = load_dataset('Open-Orca/SlimOrca-Dedup', split='train')
cot = load_dataset('kaist-ai/CoT-Collection', split='train')
physics = load_dataset('camel-ai/physics', split='train')
chemistry = load_dataset('camel-ai/chemistry', split='train')
```

---

## Data Quality Notes

1. **OpenMathInstruct-2**: Very high quality, but check for contamination with eval sets
2. **SlimOrca**: GPT-4 quality filtered, but may contain refusals
3. **CoT-Collection**: Academic quality, good diversity
4. **CAMEL datasets**: GPT-4 generated, may contain errors - consider verification
5. **OpenThoughts3**: Recent (2025), QwQ-32B traces, trending on HF

---

*Last updated: January 2025*
